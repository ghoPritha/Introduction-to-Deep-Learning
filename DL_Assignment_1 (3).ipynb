{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/MyDrive/Colab Notebooks\")"
      ],
      "metadata": {
        "id": "53XXnPTTAWRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCksCPnjQxE2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "6ee8681b-fa66-4170-ef5e-6896c4d8b53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN8UlEQVR4nO3dfahc9Z3H8c/HtI3gFZI0GGLqrrWoJCnELiEGV5cukpr1Hy1IqMrqutL4h0EFEcX9w6islmV1EQOFW3xITdcg+JTUYnVDWV2QksTHaNb6EGMS8rAhoAmi9Sbf/eOeyK3e+c3NzJk5k/t9v+AyM+c7Z86XQz45T3Pm54gQgMnvhKYbANAfhB1IgrADSRB2IAnCDiTxrX4uzDan/oEeiwiPN72rLbvtpbbftf2+7du6+SwAveVOr7PbniLpT5KWSNopaaOkyyPincI8bNmBHuvFln2RpPcj4sOI+LOktZIu6eLzAPRQN2GfI2nHmNc7q2l/wfZy25tsb+piWQC61PMTdBExLGlYYjceaFI3W/Zdkk4b8/p71TQAA6ibsG+UdKbt79v+jqSfSVpXT1sA6tbxbnxEjNheIen3kqZIejgi3q6tMwC16vjSW0cL45gd6LmefKkGwPGDsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ6HrIZx4cpU6YU69OnT+/p8leuXNmyNjQ0VJx33rx5xfpll11WrK9Zs6Zl7YILLijOOzIyUqwPDw8X69dff32x3oSuwm77I0kHJR2WNBIRC+toCkD96tiy/31E7K/hcwD0EMfsQBLdhj0kvWB7s+3l473B9nLbm2xv6nJZALrQ7W78+RGxy/Ypkl60/b8R8dLYN0TEsKRhSbIdXS4PQIe62rJHxK7qcZ+kpyUtqqMpAPXrOOy2T7J98tHnkn4iaUtdjQGoVze78bMkPW376Of8Z0Q8X0tXk8wZZ5xRrJ944onF+kUXXVSsL1mypGVt2rRpxXkXL15crDfp008/LdafeOKJYn3RotY7ml988UVx3h07dhTrGzZsKNYHUcdhj4gPJS2osRcAPcSlNyAJwg4kQdiBJAg7kARhB5JwRP++1DZZv0HX7nbJF154oVifOnVqne0cN9r927v55puL9UOHDnW87HaX1vbs2VOsv/HGGx0vu9ciwuNNZ8sOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwnb0GM2fOLNbffffdYr3XP+fcjW3bthXrBw8eLNbnz5/fsnb48OHivO1u/cX4uM4OJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0kwZHMN9u8vj2t5yy23FOvLli0r1l955ZVi/Y477ijWS3bu3FmsL1hQ/gHhdveUL1zYemDfu+66qzgv6sWWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4H72AdBuWOVPPvmkWH/uueda1pYuXVqc98YbbyzWH3zwwWIdg6fj+9ltP2x7n+0tY6bNsP2i7feqx8H99QUAkia2G/+opK9vHm6TtCEizpS0oXoNYIC1DXtEvCTpwNcmXyJpdfV8taRLa+4LQM06/W78rIjYXT3fI2lWqzfaXi5peYfLAVCTrm+EiYgonXiLiGFJwxIn6IAmdXrpba/t2ZJUPe6rryUAvdBp2NdJurp6frWkZ+tpB0CvtL3ObvtxST+WNFPSXkl3SHpG0hOS/krSdknLIuLrJ/HG+yx243tgzZo1LWtXXHFFcd52v2lf+t13STpy5Eixjv5rdZ297TF7RFzeonRhVx0B6Cu+LgskQdiBJAg7kARhB5Ig7EAS3OI6CQwNDbWsbdy4sTjv2WefXay3u3S3du3aYh39x5DNQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE19knublz5xbrr732WrH++eefF+ubN28u1l9++eWWtTvvvLM4bz//bU4mXGcHkiPsQBKEHUiCsANJEHYgCcIOJEHYgSS4zp7ctddeW6yvWrWqWJ86dWrHy77//vuL9QceeKBY37FjR8fLnsy4zg4kR9iBJAg7kARhB5Ig7EAShB1IgrADSXCdHUXnnntusf7QQw8V6/Pmzet42evXry/Wb7jhhmJ9+/btHS/7eNbxdXbbD9veZ3vLmGkrbe+y/Xr1d3GdzQKo30R24x+VtHSc6f8REedUf7+rty0AdWsb9oh4SdKBPvQCoIe6OUG3wvab1W7+9FZvsr3c9ibbm7pYFoAudRr2X0r6gaRzJO2WdF+rN0bEcEQsjIiFHS4LQA06CntE7I2IwxFxRNKvJC2qty0Adeso7LZnj3n5U0lbWr0XwGBoe53d9uOSfixppqS9ku6oXp8jKSR9JOm6iNjddmFcZ590ZsyYUaxfddVVLWv33dfy6E+SZI97ufgrW7duLdbnz59frE9Wra6zf2sCM14+zuTyNykADBy+LgskQdiBJAg7kARhB5Ig7EAS3OKKxoyMjBTrJ5xQ3hYdOXKkWF+2bFnL2lNPPVWc93jGT0kDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJt73pDbosXLy7Wr7nmmo7nb3cdvZ09e/YU688880xXnz/ZsGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSS4zj7JLViwoFhfuXJlsX7hhRcW60NDQ8fa0oS1u199//79Xc2fDVt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC6+zHgTlz5hTrK1asaFm77rrrivNOmzato57q8PHHHxfr7b4D8Oijj9bXTAJtt+y2T7P9B9vv2H7b9o3V9Bm2X7T9XvU4vfftAujURHbjRyTdHBHzJC2WdL3teZJuk7QhIs6UtKF6DWBAtQ17ROyOiFer5wclbZU0R9IlklZXb1st6dJeNQmge8d0zG77dEk/kvRHSbMiYndV2iNpVot5lkta3nmLAOow4bPxtockPSnppoj4dGwtRkeHHHfQxogYjoiFEbGwq04BdGVCYbf9bY0G/TcRcXT4y722Z1f12ZL29aZFAHVouxtv25IekrQ1Iu4fU1on6WpJv6gen+1Jh5PAqaeeWqyfd955xfqqVauK9VNOOeWYe6rLtm3bivV77rmnZe2RRx4pzsstqvWayDH730r6R0lv2X69mna7RkP+hO1rJW2X1HowbACNaxv2iPgfSeMO7i6p/MsGAAYGX5cFkiDsQBKEHUiCsANJEHYgCW5xnaCZM2e2rK1fv74471lnnVWsT5/e3A2DH3zwQbF+7733Futr164t1j/77LNj7gm9wZYdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JIc519yZIlxfrdd99drM+dO7dl7eSTT+6op7p8+eWXLWuPPfZYcd6bbrqpWD906FBHPWHwsGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSTSXGe/8sori/VFixb1bNl79+4t1p9//vlifWRkpFi/9dZbW9YOHDhQnBd5sGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQcEeU32KdJ+rWkWZJC0nBEPGB7paSfS/q/6q23R8Tv2nxWeWEAuhYR4466PJGwz5Y0OyJetX2ypM2SLtXoeOyHIuLfJ9oEYQd6r1XYJzI++25Ju6vnB21vlTSn3vYA9NoxHbPbPl3SjyT9sZq0wvabth+2Pe4YRraX295ke1NXnQLoStvd+K/eaA9J+m9J/xoRT9meJWm/Ro/j79borv4/t/kMduOBHuv4mF2SbH9b0m8l/T4i7h+nfrqk30bED9t8DmEHeqxV2Nvuxtu2pIckbR0b9OrE3VE/lbSl2yYB9M5EzsafL+llSW9JOlJNvl3S5ZLO0ehu/EeSrqtO5pU+iy070GNd7cbXhbADvdfxbjyAyYGwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRL+HbN4vafuY1zOraYNoUHsb1L4keutUnb39datCX+9n/8bC7U0RsbCxBgoGtbdB7Uuit071qzd244EkCDuQRNNhH254+SWD2tug9iXRW6f60lujx+wA+qfpLTuAPiHsQBKNhN32Utvv2n7f9m1N9NCK7Y9sv2X79abHp6vG0Ntne8uYaTNsv2j7vepx3DH2Guptpe1d1bp73fbFDfV2mu0/2H7H9tu2b6ymN7ruCn31Zb31/Zjd9hRJf5K0RNJOSRslXR4R7/S1kRZsfyRpYUQ0/gUM238n6ZCkXx8dWsv2v0k6EBG/qP6jnB4Rtw5Ibyt1jMN496i3VsOM/5MaXHd1Dn/eiSa27IskvR8RH0bEnyWtlXRJA30MvIh4SdKBr02+RNLq6vlqjf5j6bsWvQ2EiNgdEa9Wzw9KOjrMeKPrrtBXXzQR9jmSdox5vVODNd57SHrB9mbby5tuZhyzxgyztUfSrCabGUfbYbz76WvDjA/Muutk+PNucYLum86PiL+R9A+Srq92VwdSjB6DDdK1019K+oFGxwDcLem+Jpuphhl/UtJNEfHp2FqT626cvvqy3poI+y5Jp415/b1q2kCIiF3V4z5JT2v0sGOQ7D06gm71uK/hfr4SEXsj4nBEHJH0KzW47qphxp+U9JuIeKqa3Pi6G6+vfq23JsK+UdKZtr9v+zuSfiZpXQN9fIPtk6oTJ7J9kqSfaPCGol4n6erq+dWSnm2wl78wKMN4txpmXA2vu8aHP4+Ivv9JulijZ+Q/kPQvTfTQoq8zJL1R/b3ddG+SHtfobt2XGj23ca2k70raIOk9Sf8lacYA9faYRof2flOjwZrdUG/na3QX/U1Jr1d/Fze97gp99WW98XVZIAlO0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEv8PeyZ6Oei43w0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import MNISTDataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "plt.imshow(train_images[0], cmap=\"Greys_r\")\n",
        "\n",
        "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here is an attempt at illustrating what flattening looks like\n",
        "reshaped = train_images[155].reshape((1, 28*28))\n",
        "plt.figure(figsize=(15, 0.1))\n",
        "plt.pcolormesh(reshaped, cmap=\"Greys_r\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CDgE5rnk5eJU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "72ae5829-dbd2-4fbc-89a3-ffab8acd1a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x7.2 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_images.shape, train_labels.shape)"
      ],
      "metadata": {
        "id": "lk_HB5JM5gXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afdd9b5-e096-429a-f4fd-0d33367f3c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the linear model\n",
        "# weight dimensions are completely determined by input/output dimensions\n",
        "# W = tf.Variable(np.zeros([28*28, 10]).astype(np.float32))\n",
        "# b = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "Weight_0 = tf.Variable(tf.random.uniform(shape = [28*28, 256], minval = -0.1, maxval = 0.1, dtype = tf.dtypes.float32))\n",
        "# bias_0 = tf.Variable(tf.random.normal([128], dtype=np.float32))\n",
        "bias_0 = tf.Variable(np.zeros(256, dtype=np.float32))\n",
        "# Weight_1 = tf.Variable(tf.random.uniform(shape = [512, 128], minval = -0.1, maxval = 0.1, dtype = tf.dtypes.float32))\n",
        "# bias_1 = tf.Variable(tf.random.normal([128], dtype=np.float32))\n",
        "Weight_2 = tf.Variable(tf.random.uniform(shape = [256, 10], minval = -0.1, maxval = 0.1, dtype = tf.dtypes.float32))\n",
        "# bias_2 = tf.Variable(tf.random.normal([10], dtype=np.float32))\n",
        "bias_2 = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "\n",
        "def model(inputs):\n",
        "    output_0 = tf.nn.relu(tf.matmul(inputs, Weight_0) + bias_0)\n",
        "    # output_1 = tf.nn.relu(tf.matmul(output_0, Weight_1) + bias_1)\n",
        "    output_2 = tf.nn.relu(tf.matmul(output_0, Weight_2) + bias_2)\n",
        "    return output_2\n",
        "\n",
        "# NOTE\n",
        "# the ONLY thing that you should have to change for a working MLP is to\n",
        "# - add more variables\n",
        "# - adapt the model function (don't forget activation functions)\n",
        "# - add the new variables in the gradient call/update below\n",
        "\n",
        "# parameters for the training process\n",
        "# this already offers potential for experimentation\n",
        "# - how many steps do we actually need to reach acceptable performance?\n",
        "# - what if we train for some absurd number of steps?\n",
        "# - what happens if we increase/decrease the learning rate?\n",
        "# - do learning rate and number of steps interact?\n",
        "# - ...\n",
        "train_steps = 6000\n",
        "learning_rate = 0.5"
      ],
      "metadata": {
        "id": "FpxVBGo05jfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for step in range(train_steps+1):\n",
        "    image_batch, label_batch = data.next_batch()\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits = model(image_batch)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=label_batch))\n",
        "        \n",
        "    grads_0 = tape.gradient(xent, [Weight_0, bias_0])\n",
        "    Weight_0.assign_sub(learning_rate * grads_0[0])\n",
        "    bias_0.assign_sub(learning_rate * grads_0[1])\n",
        "\n",
        "    # grads_1 = tape.gradient(xent, [Weight_1, bias_1])\n",
        "    # Weight_1.assign_sub(learning_rate * grads_1[0])\n",
        "    # bias_1.assign_sub(learning_rate * grads_1[1])\n",
        "\n",
        "    grads_2 = tape.gradient(xent, [Weight_2, bias_2])\n",
        "    Weight_2.assign_sub(learning_rate * grads_2[0])\n",
        "    bias_2.assign_sub(learning_rate * grads_2[1])  \n",
        "\n",
        "    del tape\n",
        "    # every so often we print loss/accuracy\n",
        "    if not step % 500:\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch),\n",
        "                             tf.float32))\n",
        "        print(\"Step {}. Batch loss: {} Batch accuracy: {}\".format(step+1, xent, acc))"
      ],
      "metadata": {
        "id": "avSyb8NC5mDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3094320e-50d9-4fac-c19f-28eb2fd458e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1. Batch loss: 2.339751720428467 Batch accuracy: 0.078125\n",
            "Starting new epoch...\n",
            "Step 501. Batch loss: 0.0949556902050972 Batch accuracy: 0.984375\n",
            "Starting new epoch...\n",
            "Step 1001. Batch loss: 0.06079687550663948 Batch accuracy: 0.984375\n",
            "Starting new epoch...\n",
            "Step 1501. Batch loss: 0.042873136699199677 Batch accuracy: 0.984375\n",
            "Starting new epoch...\n",
            "Step 2001. Batch loss: 0.04424075782299042 Batch accuracy: 0.984375\n",
            "Starting new epoch...\n",
            "Step 2501. Batch loss: 0.021050531417131424 Batch accuracy: 1.0\n",
            "Starting new epoch...\n",
            "Step 3001. Batch loss: 0.05318629741668701 Batch accuracy: 0.984375\n",
            "Starting new epoch...\n",
            "Step 3501. Batch loss: 0.009625637903809547 Batch accuracy: 1.0\n",
            "Starting new epoch...\n",
            "Step 4001. Batch loss: 0.007542750798165798 Batch accuracy: 1.0\n",
            "Starting new epoch...\n",
            "Step 4501. Batch loss: 0.015106895007193089 Batch accuracy: 1.0\n",
            "Starting new epoch...\n",
            "Step 5001. Batch loss: 0.014659600332379341 Batch accuracy: 0.9921875\n",
            "Starting new epoch...\n",
            "Step 5501. Batch loss: 0.006277117878198624 Batch accuracy: 1.0\n",
            "Starting new epoch...\n",
            "Step 6001. Batch loss: 0.013748785480856895 Batch accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = tf.argmax(model(data.test_data), axis=1,\n",
        "                       output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(\"Test accuracy using Relu: {}\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhBshQ1L5onM",
        "outputId": "cb7519c8-656a-4457-8fe1-7ad858aad5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy using Relu: 0.9800000190734863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1> step-size effects accuracy ->\n",
        "*   2000 steps results in an accuracy of ~ 95%\n",
        "*   5000 steps results in an accuracy of ~ 96%\n",
        "*   5000 steps results in an accuracy of ~ 97.35%\n",
        "\n",
        "3> Learning rate effects ->\n",
        "*   step = 4000, LR = 0.05 , accuracy ~ 96.1 (increased training time, minute weight updation takes place)\n",
        "*   step = 4000, LR = 0.5 , accuracy ~ 86.87 (drops, high LR skips local minima )\n",
        "\n",
        "4> Learning rate &  number of steps are inter-dependent\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Oy2LJ49j9Woo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the linear model\n",
        "# weight dimensions are completely determined by input/output dimensions\n",
        "\n",
        "Weight_0 = tf.Variable(tf.random.uniform(shape = [28*28, 256], minval = -0.1, maxval = 0.1, dtype = tf.dtypes.float32))\n",
        "bias_0 = tf.Variable(np.zeros(256, dtype=np.float32))\n",
        "Weight_2 = tf.Variable(tf.random.uniform(shape = [256, 10], minval = -0.1, maxval = 0.1, dtype = tf.dtypes.float32))\n",
        "bias_2 = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "\n",
        "def model(inputs):\n",
        "    output_0 = tf.nn.softmax(tf.matmul(inputs, Weight_0) + bias_0)\n",
        "    output_2 = tf.nn.softmax(tf.matmul(output_0, Weight_2) + bias_2)\n",
        "    return output_2\n",
        "\n",
        "# NOTE\n",
        "# the ONLY thing that you should have to change for a working MLP is to\n",
        "# - add more variables\n",
        "# - adapt the model function (don't forget activation functions)\n",
        "# - add the new variables in the gradient call/update below\n",
        "\n",
        "# parameters for the training process\n",
        "# this already offers potential for experimentation\n",
        "# - how many steps do we actually need to reach acceptable performance?\n",
        "# - what if we train for some absurd number of steps?\n",
        "# - what happens if we increase/decrease the learning rate?\n",
        "# - do learning rate and number of steps interact?\n",
        "# - ...\n",
        "train_steps = 4000\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "t7aF5WRp8hxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for step in range(train_steps+1):\n",
        "    image_batch, label_batch = data.next_batch()\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits = model(image_batch)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=label_batch))\n",
        "        \n",
        "    grads_0 = tape.gradient(xent, [Weight_0, bias_0])\n",
        "    Weight_0.assign_sub(learning_rate * grads_0[0])\n",
        "    bias_0.assign_sub(learning_rate * grads_0[1])\n",
        "\n",
        "    # grads_1 = tape.gradient(xent, [Weight_1, bias_1])\n",
        "    # Weight_1.assign_sub(learning_rate * grads_1[0])\n",
        "    # bias_1.assign_sub(learning_rate * grads_1[1])\n",
        "\n",
        "    grads_2 = tape.gradient(xent, [Weight_2, bias_2])\n",
        "    Weight_2.assign_sub(learning_rate * grads_2[0])\n",
        "    bias_2.assign_sub(learning_rate * grads_2[1])  \n",
        "\n",
        "    del tape\n",
        "    # every so often we print loss/accuracy\n",
        "    if not step % 500:\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch),\n",
        "                             tf.float32))\n",
        "        print(\"Step {}. Batch loss: {} Batch accuracy: {}\".format(step+1, xent, acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e1c388-3d23-403c-bc70-d697948148cb",
        "id": "MORZU8_E8p3i"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1. Batch loss: 2.3025689125061035 Batch accuracy: 0.0625\n",
            "Starting new epoch...\n",
            "Starting new epoch...\n",
            "Step 501. Batch loss: 2.3027420043945312 Batch accuracy: 0.0520833320915699\n",
            "Starting new epoch...\n",
            "Step 1001. Batch loss: 2.3018712997436523 Batch accuracy: 0.1328125\n",
            "Starting new epoch...\n",
            "Step 1501. Batch loss: 2.3018949031829834 Batch accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Step 2001. Batch loss: 2.301546573638916 Batch accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Step 2501. Batch loss: 2.3033430576324463 Batch accuracy: 0.0859375\n",
            "Starting new epoch...\n",
            "Step 3001. Batch loss: 2.304590940475464 Batch accuracy: 0.0703125\n",
            "Starting new epoch...\n",
            "Step 3501. Batch loss: 2.3024563789367676 Batch accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Step 4001. Batch loss: 2.3003926277160645 Batch accuracy: 0.109375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = tf.argmax(model(data.test_data), axis=1,\n",
        "                       output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(\"Test accuracy using Softmax: {}\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013e380f-d6f5-4803-a08a-2b0668e1bba5",
        "id": "7T2oBL6J8y7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy using Softmax: 0.11349999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy for Softmax activation is too low (11%), hence not usable."
      ],
      "metadata": {
        "id": "AV10hLrgxXeU"
      }
    }
  ]
}