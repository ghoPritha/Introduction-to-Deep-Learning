{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "53XXnPTTAWRW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from datetime import datetime\n",
        "import tensorboard\n",
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TASK 1***\n",
        "---------------------------------------"
      ],
      "metadata": {
        "id": "XiXtHX-g0QvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "583d1bec-e2be-4272-dd0f-85041bfc1595",
        "id": "IW_8Etujz6XO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n",
            "[4]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_set = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_images.reshape([-1, 32, 32, 3]).astype(np.float32)/255, train_labels.reshape((-1,)).astype(np.int32))).shuffle(buffer_size=60000).batch(128).repeat()\n",
        "\n",
        "test_set = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_images.reshape([-1, 32, 32, 3]).astype(np.float32)/255, test_labels.reshape((-1,)).astype(np.int32))).batch(128)\n",
        "print(train_labels[10])\n",
        "plt.imshow(train_images[10], cmap=\"Greys_r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Nars8vuIz6XQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb68381-d40c-4f4a-e5ec-95b403356947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 32, 32, 3)\n",
            "<dtype: 'float32'>\n",
            "Model: \"DenseNet3Block\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_47 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_287 (Conv2D)            (None, 32, 32, 64)   1792        ['input_47[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_288 (Conv2D)            (None, 32, 32, 64)   36928       ['conv2d_287[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_289 (Conv2D)            (None, 32, 32, 64)   36928       ['conv2d_288[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_71 (Concatenate)   (None, 32, 32, 67)   0           ['conv2d_289[0][0]',             \n",
            "                                                                  'input_47[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_290 (Conv2D)            (None, 32, 32, 64)   4352        ['concatenate_71[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_33 (AverageP  (None, 16, 16, 64)  0           ['conv2d_290[0][0]']             \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_291 (Conv2D)            (None, 16, 16, 64)   102464      ['average_pooling2d_33[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_292 (Conv2D)            (None, 16, 16, 64)   102464      ['conv2d_291[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_293 (Conv2D)            (None, 16, 16, 64)   102464      ['conv2d_292[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_72 (Concatenate)   (None, 16, 16, 128)  0           ['conv2d_293[0][0]',             \n",
            "                                                                  'average_pooling2d_33[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_294 (Conv2D)            (None, 16, 16, 64)   8256        ['concatenate_72[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_34 (AverageP  (None, 8, 8, 64)    0           ['conv2d_294[0][0]']             \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_295 (Conv2D)            (None, 8, 8, 64)     102464      ['average_pooling2d_34[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_296 (Conv2D)            (None, 8, 8, 64)     102464      ['conv2d_295[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_297 (Conv2D)            (None, 8, 8, 64)     102464      ['conv2d_296[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_73 (Concatenate)   (None, 8, 8, 128)    0           ['conv2d_297[0][0]',             \n",
            "                                                                  'average_pooling2d_34[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_298 (Conv2D)            (None, 8, 8, 64)     8256        ['concatenate_73[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_35 (AverageP  (None, 4, 4, 64)    0           ['conv2d_298[0][0]']             \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " global_average_pooling2d_13 (G  (None, 64)          0           ['average_pooling2d_35[0][0]']   \n",
            " lobalAveragePooling2D)                                                                           \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 10)           650         ['global_average_pooling2d_13[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 711,946\n",
            "Trainable params: 711,946\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_DenseNet = keras.Input(shape=(32, 32, 3))\n",
        "print(input_DenseNet.shape)\n",
        "print(input_DenseNet.dtype)\n",
        "\n",
        "# input_DenseNet = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(input_DenseNet)\n",
        "# input_DenseNet = layers.MaxPool2D(3, strides = 2 , padding=\"same\")(input_DenseNet)\n",
        "# Block 1\n",
        "conv_1 = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(input_DenseNet)\n",
        "conv_2 = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(conv_1)\n",
        "conv_3 = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(conv_2)\n",
        "# pool_1 = layers.MaxPool2D(3, strides = 1 , padding=\"same\")(input_DenseNet)\n",
        "\n",
        "# transition layer to reduce feature map size for Block 1\n",
        "block_1 = layers.concatenate([conv_3, input_DenseNet])\n",
        "block_1 = layers.Conv2D(64, 1, activation=\"relu\", padding=\"same\")(block_1)\n",
        "output_block_1 = layers.AveragePooling2D(pool_size=(2, 2), strides = 2, padding=\"same\")(block_1)\n",
        "\n",
        "\n",
        "# Block 2\n",
        "conv_4 = layers.Conv2D(64, 5, activation=\"relu\", padding=\"same\")(output_block_1)\n",
        "conv_5 = layers.Conv2D(64, 5, activation=\"relu\", padding=\"same\")(conv_4)\n",
        "conv_6 = layers.Conv2D(64, 5, activation=\"relu\", padding=\"same\")(conv_5)\n",
        "\n",
        "# transition layer to reduce feature map size for Block 2\n",
        "block_2 = tf.keras.layers.concatenate([conv_6, output_block_1])\n",
        "block_2 = layers.Conv2D(64, 1, activation=\"relu\", padding=\"same\")(block_2)\n",
        "output_block_2 = layers.AveragePooling2D(pool_size=(2, 2), strides = 2,  padding=\"same\")(block_2)\n",
        "\n",
        "\n",
        "# Block 3\n",
        "conv_7 = layers.Conv2D(64, 5, activation=\"relu\", padding=\"same\")(output_block_2)\n",
        "conv_8 = layers.Conv2D(64, 5, activation=\"relu\", padding=\"same\")(conv_7)\n",
        "conv_9 = layers.Conv2D(64, 5, activation=\"relu\", padding=\"same\")(conv_8)\n",
        "\n",
        "# transition layer to reduce feature map size for Block 2\n",
        "block_3 = layers.concatenate([conv_9, output_block_2])\n",
        "block_3 = layers.Conv2D(64, 1, activation=\"relu\", padding=\"same\")(block_3)\n",
        "output_block_3 = layers.AveragePooling2D(pool_size=(2, 2), strides = 2, padding=\"same\")(block_3)\n",
        "\n",
        "# output layer\n",
        "output = layers.GlobalAveragePooling2D()(output_block_3)\n",
        "output_DenseNet = layers.Dense(10)(output)\n",
        "\n",
        "\n",
        "model = keras.Model(input_DenseNet, output_DenseNet, name=\"DenseNet3Block\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz905ifBRsOx",
        "outputId": "487f7ee6-9172-46e6-c947-73d6f049a842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 25s 15ms/step - loss: 1.6331 - sparse_categorical_accuracy: 0.4118 - val_loss: 1.2804 - val_sparse_categorical_accuracy: 0.5342\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 1.0993 - sparse_categorical_accuracy: 0.6114 - val_loss: 0.9923 - val_sparse_categorical_accuracy: 0.6484\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8838 - sparse_categorical_accuracy: 0.6913 - val_loss: 0.8407 - val_sparse_categorical_accuracy: 0.7069\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7531 - sparse_categorical_accuracy: 0.7377 - val_loss: 0.7449 - val_sparse_categorical_accuracy: 0.7444\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6533 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.7725 - val_sparse_categorical_accuracy: 0.7479\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics= [tf.metrics.SparseCategoricalAccuracy()])\n",
        "history = model.fit(train_images, train_labels, epochs=5, \n",
        "                    validation_data=(test_images,test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "hiEZNMPiUhXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36792fe2-54da-418e-e0f6-8e0636ff92e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 0.7725 - sparse_categorical_accuracy: 0.7479\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7725191116333008, 0.7479000091552734]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TASK 2***\n",
        "-----------------------------------\n"
      ],
      "metadata": {
        "id": "vnU_O8c_RTVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Nj1XXIVc8-2T"
      },
      "outputs": [],
      "source": [
        "# stereotypical train-step-with-function-annotation\n",
        "optimizer = tf.optimizers.Adam()\n",
        "\n",
        "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "train_acc_metric = tf.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(images)\n",
        "        xent = loss_fn(labels, logits)\n",
        "\n",
        "    variables = model.trainable_variables\n",
        "    gradients = tape.gradient(xent, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return xent, logits\n",
        "\n",
        "\n",
        "def training_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(images)\n",
        "        xent = loss_fn(labels, logits)\n",
        "\n",
        "    variables = model.trainable_variables\n",
        "    gradients = tape.gradient(xent, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return xent, logits\n",
        "\n",
        "@tf.function\n",
        "def train_step_mlp(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = mlp_model(images)\n",
        "        xent = loss_fn(labels, logits)\n",
        "\n",
        "    variables = model.trainable_variables\n",
        "    gradients = tape.gradient(xent, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return xent, logits\n",
        "\n",
        "\n",
        "def training_step_mlp(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = mlp_model(images)\n",
        "        xent = loss_fn(labels, logits)\n",
        "\n",
        "    variables = model.trainable_variables\n",
        "    gradients = tape.gradient(xent, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return xent, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTvyCwZrQkcw",
        "outputId": "77719010-b2ab-46ba-8b5f-142c2a1b61d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with graph execution\n",
            "Step no: 0 | Loss: 0.011516992002725601 | Accuracy: 0.9921875\n",
            "took 0.15270543098449707 seconds\n",
            "\n",
            "Step no: 400 | Loss: 0.02654046006500721 | Accuracy: 0.9921875\n",
            "took 14.168830394744873 seconds\n",
            "\n",
            "Step no: 800 | Loss: 0.001534186420030892 | Accuracy: 1.0\n",
            "took 14.359786748886108 seconds\n",
            "\n",
            "Step no: 1200 | Loss: 0.010468296706676483 | Accuracy: 1.0\n",
            "took 14.46867299079895 seconds\n",
            "\n",
            "Step no: 1600 | Loss: 0.002892036223784089 | Accuracy: 1.0\n",
            "took 14.433529376983643 seconds\n",
            "\n",
            "Step no: 2000 | Loss: 0.024464383721351624 | Accuracy: 0.9921875\n",
            "took 14.202605962753296 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_steps = 2000\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "print(\"with graph execution\")\n",
        "\n",
        "for step, (img_batch, lbl_batch) in enumerate(train_set):\n",
        "    if step > train_steps:\n",
        "        break\n",
        "    xent, logits = train_step(img_batch, lbl_batch)\n",
        "\n",
        "    if not step % 400:\n",
        "        train_acc_metric(lbl_batch, logits)\n",
        "        acc = train_acc_metric.result()\n",
        "        print(\"Step no: {} | Loss: {} | Accuracy: {}\".format(step, xent, acc))\n",
        "        train_acc_metric.reset_states()\n",
        "\n",
        "        stop = time.time()\n",
        "        print(\"took {} seconds\\n\".format(stop-start))\n",
        "        start = time.time()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba23a3db-062d-4f25-a774-575a9dc67e3b",
        "id": "5CEGBMXuSQVW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without graph execution\n",
            "Step no: 0 | Loss: 0.010850874707102776 | Accuracy: 0.9921875\n",
            "took 0.1514277458190918 seconds\n",
            "\n",
            "Step no: 400 | Loss: 0.01726371794939041 | Accuracy: 0.9921875\n",
            "took 18.589876413345337 seconds\n",
            "\n",
            "Step no: 800 | Loss: 0.0060083260759711266 | Accuracy: 1.0\n",
            "took 18.509227991104126 seconds\n",
            "\n",
            "Step no: 1200 | Loss: 0.07489652931690216 | Accuracy: 0.984375\n",
            "took 18.445528984069824 seconds\n",
            "\n",
            "Step no: 1600 | Loss: 0.004952406045049429 | Accuracy: 1.0\n",
            "took 18.271588563919067 seconds\n",
            "\n",
            "Step no: 2000 | Loss: 0.008561876602470875 | Accuracy: 1.0\n",
            "took 18.33797001838684 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_steps = 2000\n",
        "\n",
        "start = time.time()\n",
        "print(\"without graph execution\")\n",
        "\n",
        "for step, (img_batch, lbl_batch) in enumerate(train_set):\n",
        "    if step > train_steps:\n",
        "        break\n",
        "    \n",
        "    xent, logits = training_step(img_batch, lbl_batch)\n",
        "\n",
        "    if not step % 400:\n",
        "        train_acc_metric(lbl_batch, logits)\n",
        "        acc = train_acc_metric.result()\n",
        "        print(\"Step no: {} | Loss: {} | Accuracy: {}\".format(step, xent, acc))\n",
        "        train_acc_metric.reset_states()\n",
        "\n",
        "        stop = time.time()\n",
        "        print(\"took {} seconds\\n\".format(stop-start))\n",
        "        start = time.time()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***MLP with and without @tf.function***\n",
        "-------------------------\n"
      ],
      "metadata": {
        "id": "KzRzueRPUkyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Rescaling(1.,input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.Dense(300, activation='relu'),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax'),\n",
        "])"
      ],
      "metadata": {
        "id": "VlOlmBYhT6r1"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1930d5c8-9254-401d-8509-9795cad0a6b5",
        "id": "TOcARVKXZST4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without graph execution\n",
            "Step no: 0 | Loss: 0.014527734369039536 | Accuracy: 0.9921875\n",
            "took 0.17173504829406738 seconds\n",
            "\n",
            "Step no: 400 | Loss: 0.012718414887785912 | Accuracy: 0.9921875\n",
            "took 19.13652801513672 seconds\n",
            "\n",
            "Step no: 800 | Loss: 0.007301848381757736 | Accuracy: 1.0\n",
            "took 18.45614790916443 seconds\n",
            "\n",
            "Step no: 1200 | Loss: 0.004926498979330063 | Accuracy: 1.0\n",
            "took 18.361228704452515 seconds\n",
            "\n",
            "Step no: 1600 | Loss: 0.027607981115579605 | Accuracy: 0.984375\n",
            "took 18.43343186378479 seconds\n",
            "\n",
            "Step no: 2000 | Loss: 0.003116187173873186 | Accuracy: 1.0\n",
            "took 18.3194899559021 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_steps = 2000\n",
        "\n",
        "start = time.time()\n",
        "print(\"without graph execution\")\n",
        "\n",
        "for step, (img_batch, lbl_batch) in enumerate(train_set):\n",
        "    if step > train_steps:\n",
        "        break\n",
        "    \n",
        "    xent, logits = training_step(img_batch, lbl_batch)\n",
        "\n",
        "    if not step % 400:\n",
        "        train_acc_metric(lbl_batch, logits)\n",
        "        acc = train_acc_metric.result()\n",
        "        print(\"Step no: {} | Loss: {} | Accuracy: {}\".format(step, xent, acc))\n",
        "        train_acc_metric.reset_states()\n",
        "\n",
        "        stop = time.time()\n",
        "        print(\"took {} seconds\\n\".format(stop-start))\n",
        "        start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db74e09d-28dd-41e0-9c77-a786f3f6a77c",
        "id": "wGREwoiIZogZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with graph execution\n",
            "Step no: 0 | Loss: 0.0007358271977864206 | Accuracy: 1.0\n",
            "took 0.1582345962524414 seconds\n",
            "\n",
            "Step no: 400 | Loss: 0.005741880740970373 | Accuracy: 1.0\n",
            "took 14.379942655563354 seconds\n",
            "\n",
            "Step no: 800 | Loss: 0.004001328255981207 | Accuracy: 1.0\n",
            "took 14.569858074188232 seconds\n",
            "\n",
            "Step no: 1200 | Loss: 0.07643134146928787 | Accuracy: 0.96875\n",
            "took 14.381007432937622 seconds\n",
            "\n",
            "Step no: 1600 | Loss: 0.002633417723700404 | Accuracy: 1.0\n",
            "took 14.238472938537598 seconds\n",
            "\n",
            "Step no: 2000 | Loss: 0.03343939408659935 | Accuracy: 0.984375\n",
            "took 14.325185537338257 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_steps = 2000\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "print(\"with graph execution\")\n",
        "\n",
        "for step, (img_batch, lbl_batch) in enumerate(train_set):\n",
        "    if step > train_steps:\n",
        "        break\n",
        "    # Set up logging.\n",
        "    # stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # logdir = 'logs/func/%s' % stamp\n",
        "    # writer = tf.summary.create_file_writer(logdir)\n",
        "    # tf.summary.trace_on(graph=True, profiler=True)\n",
        "\n",
        "    xent, logits = train_step(img_batch, lbl_batch)\n",
        "\n",
        "    if not step % 400:\n",
        "        train_acc_metric(lbl_batch, logits)\n",
        "        acc = train_acc_metric.result()\n",
        "        print(\"Step no: {} | Loss: {} | Accuracy: {}\".format(step, xent, acc))\n",
        "        train_acc_metric.reset_states()\n",
        "\n",
        "        stop = time.time()\n",
        "        print(\"took {} seconds\\n\".format(stop-start))\n",
        "        start = time.time()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}